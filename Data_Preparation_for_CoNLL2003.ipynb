{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Preparation_for_CoNLL2003.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOlo2I2iydqT/03Qg4/jVTO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fackor/Advanced_Machine_Learning_Course/blob/main/Data_Preparation_for_CoNLL2003.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOB2c4_dCEWt",
        "outputId": "97a1b14d-5c17-4dae-c240-80b2ccfa2997",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers==3.5.0"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.5.0 in /usr/local/lib/python3.6/dist-packages (3.5.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (20.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.1.91)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (1.19.4)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.9.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.0) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.5.0) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5.0) (50.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAPRu53YQxn_"
      },
      "source": [
        "import itertools\n",
        "import math\n",
        "import os\n",
        "import unicodedata\n",
        "from transformers.tokenization_roberta import RobertaTokenizer"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXXRbvzGb23c"
      },
      "source": [
        "def convert_examples_to_features(\n",
        "    examples, label_list, tokenizer, max_seq_length, max_entity_length, max_mention_length\n",
        "    ):\n",
        "    max_num_subwords = max_seq_length - 2\n",
        "    label_map = {label: i for i, label in enumerate(label_list)}\n",
        "    features = []\n",
        "\n",
        "    def tokenize_word(text):\n",
        "        if (\n",
        "            isinstance(tokenizer, RobertaTokenizer)\n",
        "            and (text[0] != \"'\")\n",
        "            and (len(text) != 1 or not is_punctuation(text))\n",
        "        ):\n",
        "            return tokenizer.tokenize(text, add_prefix_space=True)\n",
        "        return tokenizer.tokenize(text)\n",
        "\n",
        "    for example_index, example in enumerate(examples):\n",
        "        tokens = [tokenize_word(w) for w in example.words]\n",
        "        subwords = [w for li in tokens for w in li]\n",
        "\n",
        "        subword2token = list(itertools.chain(*[[i] * len(li) for i, li in enumerate(tokens)]))\n",
        "        token2subword = [0] + list(itertools.accumulate(len(li) for li in tokens))\n",
        "        subword_start_positions = frozenset(token2subword)\n",
        "        subword_sentence_boundaries = [sum(len(li) for li in tokens[:p]) for p in example.sentence_boundaries]\n",
        "\n",
        "        entity_labels = {}\n",
        "        start = None\n",
        "        cur_type = None\n",
        "        for n, label in enumerate(example.labels):\n",
        "            if label == \"O\" or n in example.sentence_boundaries:\n",
        "                if start is not None:\n",
        "                    entity_labels[(token2subword[start], token2subword[n])] = label_map[cur_type]\n",
        "                    start = None\n",
        "                    cur_type = None\n",
        "\n",
        "            if label.startswith(\"B\"):\n",
        "                if start is not None:\n",
        "                    entity_labels[(token2subword[start], token2subword[n])] = label_map[cur_type]\n",
        "                start = n\n",
        "                cur_type = label[2:]\n",
        "\n",
        "            elif label.startswith(\"I\"):\n",
        "                if start is None:\n",
        "                    start = n\n",
        "                    cur_type = label[2:]\n",
        "                elif cur_type != label[2:]:\n",
        "                    entity_labels[(token2subword[start], token2subword[n])] = label_map[cur_type]\n",
        "                    start = n\n",
        "                    cur_type = label[2:]\n",
        "\n",
        "        if start is not None:\n",
        "            entity_labels[(token2subword[start], len(subwords))] = label_map[cur_type]\n",
        "\n",
        "        for n in range(len(subword_sentence_boundaries) - 1):\n",
        "            doc_sent_start, doc_sent_end = subword_sentence_boundaries[n : n + 2]\n",
        "\n",
        "            left_length = doc_sent_start\n",
        "            right_length = len(subwords) - doc_sent_end\n",
        "            sentence_length = doc_sent_end - doc_sent_start\n",
        "            half_context_length = int((max_num_subwords - sentence_length) / 2)\n",
        "\n",
        "            if left_length < right_length:\n",
        "                left_context_length = min(left_length, half_context_length)\n",
        "                right_context_length = min(right_length, max_num_subwords - left_context_length - sentence_length)\n",
        "            else:\n",
        "                right_context_length = min(right_length, half_context_length)\n",
        "                left_context_length = min(left_length, max_num_subwords - right_context_length - sentence_length)\n",
        "\n",
        "            doc_offset = doc_sent_start - left_context_length\n",
        "            target_tokens = subwords[doc_offset : doc_sent_end + right_context_length]\n",
        "\n",
        "            word_ids = tokenizer.convert_tokens_to_ids([tokenizer.cls_token] + target_tokens + [tokenizer.sep_token])\n",
        "            word_attention_mask = [1] * (len(target_tokens) + 2)\n",
        "            word_segment_ids = [0] * (len(target_tokens) + 2)\n",
        "\n",
        "            entity_start_positions = []\n",
        "            entity_end_positions = []\n",
        "            entity_ids = []\n",
        "            entity_attention_mask = []\n",
        "            entity_segment_ids = []\n",
        "            entity_position_ids = []\n",
        "            original_entity_spans = []\n",
        "            labels = []\n",
        "\n",
        "            for entity_start in range(left_context_length, left_context_length + sentence_length):\n",
        "                doc_entity_start = entity_start + doc_offset\n",
        "                if doc_entity_start not in subword_start_positions:\n",
        "                    continue\n",
        "                for entity_end in range(entity_start + 1, left_context_length + sentence_length + 1):\n",
        "                    doc_entity_end = entity_end + doc_offset\n",
        "                    if doc_entity_end not in subword_start_positions:\n",
        "                        continue\n",
        "\n",
        "                    if entity_end - entity_start > max_mention_length:\n",
        "                        continue\n",
        "\n",
        "                    entity_start_positions.append(entity_start + 1)\n",
        "                    entity_end_positions.append(entity_end)\n",
        "                    entity_ids.append(1)\n",
        "                    entity_attention_mask.append(1)\n",
        "                    entity_segment_ids.append(0)\n",
        "\n",
        "                    position_ids = list(range(entity_start + 1, entity_end + 1))\n",
        "                    position_ids += [-1] * (max_mention_length - entity_end + entity_start)\n",
        "                    entity_position_ids.append(position_ids)\n",
        "\n",
        "                    original_entity_spans.append(\n",
        "                        (subword2token[doc_entity_start], subword2token[doc_entity_end - 1] + 1)\n",
        "                    )\n",
        "\n",
        "                    labels.append(entity_labels.get((doc_entity_start, doc_entity_end), 0))\n",
        "                    entity_labels.pop((doc_entity_start, doc_entity_end), None)\n",
        "\n",
        "            if len(entity_ids) == 1:\n",
        "                entity_start_positions.append(0)\n",
        "                entity_end_positions.append(0)\n",
        "                entity_ids.append(0)\n",
        "                entity_attention_mask.append(0)\n",
        "                entity_segment_ids.append(0)\n",
        "                entity_position_ids.append(([-1] * max_mention_length))\n",
        "                original_entity_spans.append(None)\n",
        "                labels.append(-1)\n",
        "\n",
        "            split_size = math.ceil(len(entity_ids) / max_entity_length)\n",
        "            for i in range(split_size):\n",
        "                entity_size = math.ceil(len(entity_ids) / split_size)\n",
        "                start = i * entity_size\n",
        "                end = start + entity_size\n",
        "                features.append(\n",
        "                    InputFeatures(\n",
        "                        example_index=example_index,\n",
        "                        word_ids=word_ids,\n",
        "                        word_attention_mask=word_attention_mask,\n",
        "                        word_segment_ids=word_segment_ids,\n",
        "                        entity_start_positions=entity_start_positions[start:end],\n",
        "                        entity_end_positions=entity_end_positions[start:end],\n",
        "                        entity_ids=entity_ids[start:end],\n",
        "                        entity_position_ids=entity_position_ids[start:end],\n",
        "                        entity_segment_ids=entity_segment_ids[start:end],\n",
        "                        entity_attention_mask=entity_attention_mask[start:end],\n",
        "                        original_entity_spans=original_entity_spans[start:end],\n",
        "                        labels=labels[start:end],\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        assert not entity_labels\n",
        "\n",
        "    return features"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ytvBLB6p8or"
      },
      "source": [
        "class InputFeatures(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        example_index,\n",
        "        word_ids,\n",
        "        word_segment_ids,\n",
        "        word_attention_mask,\n",
        "        entity_start_positions,\n",
        "        entity_end_positions,\n",
        "        entity_ids,\n",
        "        entity_position_ids,\n",
        "        entity_segment_ids,\n",
        "        entity_attention_mask,\n",
        "        original_entity_spans,\n",
        "        labels,\n",
        "    ):\n",
        "        self.example_index = example_index\n",
        "        self.word_ids = word_ids\n",
        "        self.word_segment_ids = word_segment_ids\n",
        "        self.word_attention_mask = word_attention_mask\n",
        "        self.entity_start_positions = entity_start_positions\n",
        "        self.entity_end_positions = entity_end_positions\n",
        "        self.entity_ids = entity_ids\n",
        "        self.entity_position_ids = entity_position_ids\n",
        "        self.entity_segment_ids = entity_segment_ids\n",
        "        self.entity_attention_mask = entity_attention_mask\n",
        "        self.original_entity_spans = original_entity_spans\n",
        "        self.labels = labels"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLExCeGXc95O"
      },
      "source": [
        "def read_data(input_file):\n",
        "    data = []\n",
        "    words = []\n",
        "    labels = []\n",
        "    sentence_boundaries = []\n",
        "    with open(input_file) as f:\n",
        "        for line in f:\n",
        "            line = line.rstrip()\n",
        "            if line.startswith(\"-DOCSTART\"):\n",
        "                if words:\n",
        "                    data.append((words, labels, sentence_boundaries))\n",
        "                    assert sentence_boundaries[0] == 0\n",
        "                    assert sentence_boundaries[-1] == len(words)\n",
        "                    words = []\n",
        "                    labels = []\n",
        "                    sentence_boundaries = []\n",
        "                continue\n",
        "\n",
        "            if not line:\n",
        "                if not sentence_boundaries or len(words) != sentence_boundaries[-1]:\n",
        "                    sentence_boundaries.append(len(words))\n",
        "            else:\n",
        "                parts = line.split(\" \")\n",
        "                words.append(parts[0])\n",
        "                labels.append(parts[-1])\n",
        "\n",
        "    if words:\n",
        "        data.append((words, labels, sentence_boundaries))\n",
        "\n",
        "    return data\n",
        "\n",
        "def create_examples(data, fold):\n",
        "    return [InputExample(f\"{fold}-{i}\", *args) for i, args in enumerate(data)]\n",
        "\n",
        "class InputExample(object):\n",
        "    def __init__(self, guid, words, labels, sentence_boundaries):\n",
        "        self.guid = guid\n",
        "        self.words = words\n",
        "        self.labels = labels\n",
        "        self.sentence_boundaries = sentence_boundaries\n",
        "\n",
        "def is_punctuation(char):\n",
        "    # obtained from:\n",
        "    # https://github.com/huggingface/transformers/blob/5f25a5f367497278bf19c9994569db43f96d5278/transformers/tokenization_bert.py#L489\n",
        "    cp = ord(char)\n",
        "    if (cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126):\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"P\"):\n",
        "        return True\n",
        "    return False"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlTCdIjbdyYD"
      },
      "source": [
        "data = read_data(\"/content/train.txt\")\n",
        "examples = create_examples(data, \"train\")\n",
        "\n",
        "label_list = ['O', 'PER', 'LOC', 'MISC', 'ORG']\n",
        "features = convert_examples_to_features(examples, label_list, RobertaTokenizer.from_pretrained(\"roberta-base\"), 256, 3, 30)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8NpJ4Ntfyty",
        "outputId": "b2274807-447d-4bae-a245-1dacd797ac65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "i = 0\n",
        "print(f\"EXAMPLE TEXT: {examples[i].words}\")\n",
        "\n",
        "\n",
        "print(f\"ID: {features[i].example_index}\\n\")\n",
        "print(f\"WORD_IDs: {features[i].word_ids}\\n\")\n",
        "print(f\"WORD_SEG_IDs: {features[i].word_segment_ids}\\n\")\n",
        "print(f\"WORD_ATTN_MASK: {features[i].word_attention_mask}\\n\")\n",
        "print(f\"ENT_START_POS: {features[i].entity_start_positions}\\n\")\n",
        "print(f\"ENT_END_POS: {features[i].entity_end_positions}\\n\")\n",
        "print(f\"ENT_ID: {features[i].entity_ids}\\n\")\n",
        "print(f\"ENT_POS_IDs: {features[i].entity_position_ids}\\n\")\n",
        "print(f\"ENT_SEG_IDs: {features[i].entity_segment_ids}\\n\")\n",
        "print(f\"ENT_ATTN_MASK: {features[i].entity_attention_mask}\\n\")\n",
        "print(f\"ORG_ENT_SPANS: {features[i].original_entity_spans}\\n\")\n",
        "print(f\"LABELS: {features[i].labels}\\n\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EXAMPLE TEXT: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.', 'Peter', 'Blackburn', 'BRUSSELS', '1996-08-22', 'The', 'European', 'Commission', 'said', 'on', 'Thursday', 'it', 'disagreed', 'with', 'German', 'advice', 'to', 'consumers', 'to', 'shun', 'British', 'lamb', 'until', 'scientists', 'determine', 'whether', 'mad', 'cow', 'disease', 'can', 'be', 'transmitted', 'to', 'sheep', '.', 'Germany', \"'s\", 'representative', 'to', 'the', 'European', 'Union', \"'s\", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.', '\"', 'We', 'do', \"n't\", 'support', 'any', 'such', 'recommendation', 'because', 'we', 'do', \"n't\", 'see', 'any', 'grounds', 'for', 'it', ',', '\"', 'the', 'Commission', \"'s\", 'chief', 'spokesman', 'Nikolaus', 'van', 'der', 'Pas', 'told', 'a', 'news', 'briefing', '.', 'He', 'said', 'further', 'scientific', 'study', 'was', 'required', 'and', 'if', 'it', 'was', 'found', 'that', 'action', 'was', 'needed', 'it', 'should', 'be', 'taken', 'by', 'the', 'European', 'Union', '.', 'He', 'said', 'a', 'proposal', 'last', 'month', 'by', 'EU', 'Farm', 'Commissioner', 'Franz', 'Fischler', 'to', 'ban', 'sheep', 'brains', ',', 'spleens', 'and', 'spinal', 'cords', 'from', 'the', 'human', 'and', 'animal', 'food', 'chains', 'was', 'a', 'highly', 'specific', 'and', 'precautionary', 'move', 'to', 'protect', 'human', 'health', '.', 'Fischler', 'proposed', 'EU-wide', 'measures', 'after', 'reports', 'from', 'Britain', 'and', 'France', 'that', 'under', 'laboratory', 'conditions', 'sheep', 'could', 'contract', 'Bovine', 'Spongiform', 'Encephalopathy', '(', 'BSE', ')', '--', 'mad', 'cow', 'disease', '.', 'But', 'Fischler', 'agreed', 'to', 'review', 'his', 'proposal', 'after', 'the', 'EU', \"'s\", 'standing', 'veterinary', 'committee', ',', 'mational', 'animal', 'health', 'officials', ',', 'questioned', 'if', 'such', 'action', 'was', 'justified', 'as', 'there', 'was', 'only', 'a', 'slight', 'risk', 'to', 'human', 'health', '.', 'Spanish', 'Farm', 'Minister', 'Loyola', 'de', 'Palacio', 'had', 'earlier', 'accused', 'Fischler', 'at', 'an', 'EU', 'farm', 'ministers', \"'\", 'meeting', 'of', 'causing', 'unjustified', 'alarm', 'through', '\"', 'dangerous', 'generalisation', '.', '\"', '.', 'Only', 'France', 'and', 'Britain', 'backed', 'Fischler', \"'s\", 'proposal', '.', 'The', 'EU', \"'s\", 'scientific', 'veterinary', 'and', 'multidisciplinary', 'committees', 'are', 'due', 'to', 're-examine', 'the', 'issue', 'early', 'next', 'month', 'and', 'make', 'recommendations', 'to', 'the', 'senior', 'veterinary', 'officials', '.', 'Sheep', 'have', 'long', 'been', 'known', 'to', 'contract', 'scrapie', ',', 'a', 'brain-wasting', 'disease', 'similar', 'to', 'BSE', 'which', 'is', 'believed', 'to', 'have', 'been', 'transferred', 'to', 'cattle', 'through', 'feed', 'containing', 'animal', 'waste', '.', 'British', 'farmers', 'denied', 'on', 'Thursday', 'there', 'was', 'any', 'danger', 'to', 'human', 'health', 'from', 'their', 'sheep', ',', 'but', 'expressed', 'concern', 'that', 'German', 'government', 'advice', 'to', 'consumers', 'to', 'avoid', 'British', 'lamb', 'might', 'influence', 'consumers', 'across', 'Europe', '.', '\"', 'What', 'we', 'have', 'to', 'be', 'extremely', 'careful', 'of', 'is', 'how', 'other', 'countries', 'are', 'going', 'to', 'take', 'Germany', \"'s\", 'lead', ',', '\"', 'Welsh', 'National', 'Farmers', \"'\", 'Union', '(', 'NFU', ')', 'chairman', 'John', 'Lloyd', 'Jones', 'said', 'on', 'BBC', 'radio', '.', 'Bonn', 'has', 'led', 'efforts', 'to', 'protect', 'public', 'health', 'after', 'consumer', 'confidence', 'collapsed', 'in', 'March', 'after', 'a', 'British', 'report', 'suggested', 'humans', 'could', 'contract', 'an', 'illness', 'similar', 'to', 'mad', 'cow', 'disease', 'by', 'eating', 'contaminated', 'beef', '.', 'Germany', 'imported', '47,600', 'sheep', 'from', 'Britain', 'last', 'year', ',', 'nearly', 'half', 'of', 'total', 'imports', '.', 'It', 'brought', 'in', '4,275', 'tonnes', 'of', 'British', 'mutton', ',', 'some', '10', 'percent', 'of', 'overall', 'imports', '.']\n",
            "ID: 0\n",
            "\n",
            "WORD_IDs: [0, 1281, 24020, 1859, 486, 7, 13978, 1089, 17988, 4, 2155, 20809, 6823, 16551, 16416, 8008, 12, 3669, 12, 2036, 20, 796, 1463, 26, 15, 296, 24, 19286, 19, 1859, 2949, 7, 2360, 7, 23795, 1089, 17988, 454, 4211, 3094, 549, 7758, 12094, 2199, 64, 28, 20579, 7, 14336, 4, 1600, 18, 4915, 7, 5, 796, 1332, 18, 24443, 1540, 26978, 525, 5577, 4621, 26, 15, 307, 2360, 197, 907, 14336, 38542, 31, 749, 97, 87, 1444, 454, 5, 6441, 2949, 21, 18618, 4, 113, 166, 109, 295, 75, 323, 143, 215, 6492, 142, 52, 109, 295, 75, 192, 143, 5619, 13, 24, 6, 113, 5, 1463, 18, 834, 1565, 26607, 687, 3538, 1935, 11920, 174, 10, 340, 7515, 4, 91, 26, 617, 6441, 892, 21, 1552, 8, 114, 24, 21, 303, 14, 814, 21, 956, 24, 197, 28, 551, 30, 5, 796, 1332, 4, 91, 26, 10, 2570, 94, 353, 30, 1281, 6584, 4589, 30897, 274, 13239, 1371, 7, 2020, 14336, 15813, 6, 2292, 459, 1290, 8, 21431, 37687, 31, 5, 1050, 8, 3477, 689, 9781, 21, 10, 2200, 2167, 8, 15869, 1766, 517, 7, 1744, 1050, 474, 4, 274, 13239, 1371, 1850, 1281, 12, 6445, 1797, 71, 690, 31, 1444, 8, 1470, 14, 223, 13154, 1274, 14336, 115, 1355, 163, 1417, 833, 2064, 1657, 38263, 381, 15285, 34527, 31395, 1640, 163, 3388, 43, 480, 7758, 12094, 2199, 4, 125, 274, 13239, 1371, 1507, 7, 1551, 39, 2570, 71, 5, 1281, 18, 2934, 24443, 1540, 6, 475, 5033, 3477, 474, 503, 6, 5249, 114, 2]\n",
            "\n",
            "WORD_SEG_IDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            "WORD_ATTN_MASK: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "ENT_START_POS: [1, 1, 1]\n",
            "\n",
            "ENT_END_POS: [1, 2, 3]\n",
            "\n",
            "ENT_ID: [1, 1, 1]\n",
            "\n",
            "ENT_POS_IDs: [[1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [1, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
            "\n",
            "ENT_SEG_IDs: [0, 0, 0]\n",
            "\n",
            "ENT_ATTN_MASK: [1, 1, 1]\n",
            "\n",
            "ORG_ENT_SPANS: [(0, 1), (0, 2), (0, 3)]\n",
            "\n",
            "LABELS: [4, 0, 0]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-y_B71KqVCA",
        "outputId": "f37b8fae-f572-4245-8f65-8c604f51d99d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "i = 0\n",
        "print(f\"feature_length {len(features[i].word_ids)}\\n\")\n",
        "print(f\"example length {len(examples[i].words)}\\n\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "feature_length 256\n",
            "\n",
            "example length 469\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1mgQ0QA1z55"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}