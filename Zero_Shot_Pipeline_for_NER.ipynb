{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Zero_Shot_Pipeline_for_NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fackor/Advanced_Machine_Learning_Course/blob/main/Zero_Shot_Pipeline_for_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTBP_QYuu6tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad1ff257-31c8-482e-fefc-e87a82089779"
      },
      "source": [
        "!pip install transformers==3.1.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.1.0 in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.8)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.1.94)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.8.1rc2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (20.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (3.0.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.1.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.1.0) (0.17.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.1.0) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.1.0) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiU_ES5tzpMH"
      },
      "source": [
        "from transformers import pipeline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spkccRiv0CB3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1237e488-a738-412d-878b-80a8ac14ecda"
      },
      "source": [
        "#classifier = pipeline(\"zero-shot-classification\")\n",
        "classifier = pipeline(\"zero-shot-classification\", device=0) # to utilize GPU"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
            "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8KjtUuju77u"
      },
      "source": [
        "def zeroshot_NER(paragraph, candidate_labels, threshold):\n",
        "  words = [word.lower() for word in paragraph.split() if word.isalpha()]\n",
        "  NER = []\n",
        "  for word in words:\n",
        "    output = classifier(word, candidate_labels)\n",
        "\n",
        "    label =  output['labels'][0] if output['scores'][0] > threshold else '[UNK]'\n",
        "    NER.append([word, label])\n",
        "\n",
        "  return NER  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU_gJaoWfYGL"
      },
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "candidate_labels = ['Person', 'Organization', 'Location', 'Miscellaneous', '[UNK]']\n",
        "\n",
        "label_maps = {'Person' : 'PER',\n",
        "              'Organization' : 'ORG',\n",
        "              'Location' : 'LOC',\n",
        "              'Miscellaneous': 'MISC', \n",
        "              '[UNK]' : 'O'}\n",
        "\n",
        "other_label_map = {'B-ORG' : 'ORG',\n",
        "                   'I-ORG' : 'ORG',\n",
        "                   'B-PER' : 'PER',\n",
        "                   'I-PER' : 'PER',\n",
        "                   'B-LOC' : 'LOC',\n",
        "                   'I-LOC' : 'LOC',\n",
        "                   'B-MISC' : 'MISC',\n",
        "                   'I-MISC' : 'MISC',\n",
        "                   'O' : 'O'}\n",
        "\n",
        "filename = '/content/test.txt'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUE1JsymQUWY"
      },
      "source": [
        "predictions = []\n",
        "labels = []\n",
        "\n",
        "THRESHOLD = 0.5\n",
        "with open(filename) as f:\n",
        "  for i, line in tqdm(enumerate(f)):\n",
        "    if i == 0:\n",
        "        continue\n",
        "    line = line.split()\n",
        "    if len(line) > 0 and line[0].isalpha():\n",
        "      word = line[0]\n",
        "      label = other_label_map[line[3]]\n",
        "      labels.append(label)\n",
        "      \n",
        "      out = zeroshot_NER(word, candidate_labels, THRESHOLD)\n",
        "\n",
        "      predictions.append(label_maps[out[0][1]])\n",
        "\n",
        "f1 = f1_score(labels, predictions, average='weighted')\n",
        "precision = precision_score(labels, predictions, average='weighted')\n",
        "recall = recall_score(labels, predictions, average='weighted')\n",
        "print(f\"Precision - {recall}\\nRecall - {precision}\\nf1 - {f1}\")\n",
        "'''\n",
        "Precision - 0.5756579921248187\n",
        "Recall - 0.6500091058577243\n",
        "f1 - 0.597280030256445\n",
        "'''\n",
        "\n",
        "\n",
        "# result = {}\n",
        "#'''for label in label_maps.values():\n",
        "#  pred = [x == label for x in predictions]\n",
        "#  true = [x == label for x in labels]\n",
        "#  result[label] = f1_score(true, pred, average='binary')\n",
        "\n",
        "#print(result)\n",
        "#''''''\n",
        "#50350it [22:49, 36.76it/s]\n",
        "\n",
        "#{'PER': 0.2156064461407973, 'ORG': 0.036968576709796676, 'LOC': 0.3058277462609592, 'MISC': 0.005212858384013901, 'O': 0.7298355194669998}\n",
        "#'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDYPUdOApm5_"
      },
      "source": [
        "predictions = []\n",
        "labels = []\n",
        "\n",
        "THRESHOLD = 0.6\n",
        "with open(filename) as f:\n",
        "  for i, line in tqdm(enumerate(f)):\n",
        "    if i == 0:\n",
        "        continue\n",
        "    line = line.split()\n",
        "    if len(line) > 0 and line[0].isalpha():\n",
        "      word = line[0]\n",
        "      label = other_label_map[line[3]]\n",
        "      labels.append(label)\n",
        "      \n",
        "      #print(line)\n",
        "      #print(word)\n",
        "      \n",
        "      out = zeroshot_NER(word, candidate_labels, THRESHOLD)\n",
        "\n",
        "      predictions.append(label_maps[out[0][1]])\n",
        "\n",
        "f1 = f1_score(labels, predictions, average='weighted')\n",
        "precision = precision_score(labels, predictions, average='weighted')\n",
        "recall = recall_score(labels, predictions, average='weighted')\n",
        "print(f\"Precision - {recall}\\nRecall - {precision}\\nf1 - {f1}\")\n",
        "'''\n",
        "Precision - 0.7299049649169553\n",
        "Recall - 0.6752435637801734\n",
        "f1 - 0.6949574012914023\n",
        "'''\n",
        "\n",
        "\n",
        "#'''\n",
        "#for label in label_maps.values():\n",
        "#  pred = [x == label for x in predictions]\n",
        "#  true = [x == label for x in labels]\n",
        "#  result[label] = f1_score(true, pred, average='binary')\n",
        "\n",
        "#print(result)\n",
        "\n",
        "#55044it [20:10, 45.48it/s]\n",
        "\n",
        "#{'PER': 0.21206682313958305, 'ORG': 0.04953560371517028, 'LOC': 0.35424354243542433, 'MISC': 0.0, 'O': 0.7353024911032029}\n",
        "#'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkfE6NRA0Dzy"
      },
      "source": [
        "sequence = input(\"Enter a paragraph to do NER on.\")\n",
        "candidate_labels = input(\"Enter candidate labels separated by double colon(::).\").split(\"::\")\n",
        "candidate_labels.append('[UNK]')\n",
        "\n",
        "NER = zeroshot_NER(sequence, candidate_labels, 0.4)\n",
        "for each in NER:\n",
        "  print(f\"{each[0]}<{each[1]}>\", end=\" \")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}